---
name: big data
---
Big data is a term that refers to tasks that have enough data processing to require different programming techniques to be able to achieve results. Traditionally this has been used to refer to dealing with data that's larger than the amount of RAM or memory accessible on a single machine.

Informally this term is used in many contexts, from data too big to fit in a spreadsheet, to data too big to fit on a standard operating environment (SOE) laptops RAM capacity. If a project has enough data such that you are forced to make technical changes to accommodate for it then you still may not have a Big Data problem, there's multiple approaches that you can try first. It's only when a task is completely infeasible to run on one server that you have to investigate heavy duty Big Data techniques.

Due to the rapid decrease in cost of memory (and processor power) over the years many situations that were once Big Data problems can now fit on a single server instance. We have been involved in projects where we provisioned servers with a terabyte of RAM in order to do in memory processing of data. Sometimes a bigger server is cheaper than the costs associated with the software engineering required to go with a legitimate Big Data pipeline and sometimes it's not. We can help you with these sorts of decisions, feel free to [contact us](/contact) to discuss your needs in this space.
